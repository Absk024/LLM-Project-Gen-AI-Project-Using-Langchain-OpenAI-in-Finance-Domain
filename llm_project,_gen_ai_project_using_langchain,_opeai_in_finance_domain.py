# -*- coding: utf-8 -*-
"""LLM Project, Gen AI project using Langchain, OpeAI in Finance Domain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o8fi6NMp6jiQIbnBeTm2BL3VhXD8DgFw
"""

!pip install langchain

pip install -U langchain-community

from langchain.document_loaders import TextLoader

loader = TextLoader("/content/nvda_news_1.txt")
data = loader.load()
data

data[0].page_content

data[0].metadata

from langchain.document_loaders.csv_loader import CSVLoader

Loader = CSVLoader("/content/movies.csv", source_column="title")
data = Loader.load()
len(data)

data[0]

data[0].metadata

!pip install unstructured libmagic python-magic python-magic-bin

from langchain.document_loaders import UnstructuredURLLoader

!pip install unstructured

loader = UnstructuredURLLoader(urls=["https://www.youtube.com/shorts/So_RbDE9cXM"])

data = loader.load()
len(data)

data[0].metadata

text = """In an age where environmental sustainability is paramount, Anagha Cleaners, founded by Abhishek

Kumar, has taken significant strides towards reducing the ecological footprint of household cleaning

products. Our commitment to natural, eco-friendly cleaning solutions addresses several pressing

environmental problems.

Traditional Cleaners and Environmental Damage

Conventional cleaning products often contain harmful chemicals like phosphates, chlorine, and

synthetic fragrances. When these products are washed down the drain, they can contaminate water

sources, harm aquatic life, and contribute to air pollution. The production and disposal of these

products also add to the planet’s burden of non-biodegradable waste.

Eco-Friendly Ingredients

Anagha Cleaners are formulated using natural, biodegradable ingredients that do not harm the

environment. By avoiding phosphates, ammonia, and synthetic chemicals, our products ensure that

no toxic substances are released into waterways. This helps protect aquatic ecosystems from the

damaging effects of conventional cleaners.

Reducing Plastic Waste

Our commitment to sustainability extends beyond our products to our packaging. Anagha Cleaners

utilize recyclable and refillable packaging to minimize plastic waste. By encouraging customers to

reuse containers, we help reduce the amount of plastic that ends up in landfills and oceans,

contributing to a cleaner planet.

Supporting Biodiversity

By choosing natural ingredients that are safe for the environment, Anagha Cleaners support

biodiversity. Our products do not contain chemicals that can disrupt ecosystems or harm wildlife.

This ensures that both terrestrial and aquatic habitats remain healthy and vibrant, promoting the

well-being of countless species.

Consumer Awareness and Education

Anagha Cleaners is dedicated to raising awareness about the environmental impact of traditional

cleaning products. Through our educational initiatives, we empower consumers to make informed

choices that benefit both their homes and the planet. By promoting sustainable living practices, we

aim to inspire a broader movement towards environmental responsibility.

Sustainable Production Practices

Our commitment to the environment is reflected in our production practices. Anagha Cleaners use

sustainable sourcing methods and energy-efficient manufacturing processes. This reduces our carbon

footprint and ensures that our operations have a minimal impact on the environment.

Conclusion

Anagha Cleaners is more than just a brand; it’s a movement towards a healthier planet. By offering

natural, eco-friendly cleaning solutions, we address significant environmental problems associated

with traditional cleaners. From reducing plastic waste to protecting aquatic life, our products contribute to a sustainable future. Choose Anagha Cleaners and join us in our mission to safeguard

the environment for generations to come. """

from langchain.text_splitter import CharacterTextSplitter

splitter = CharacterTextSplitter(separator="\n", chunk_size=200, chunk_overlap=0)

chunks = splitter.split_text(text)
len(chunks)

chunks

for chunk in chunks:
  print(len(chunk))

from langchain.text_splitter import RecursiveCharacterTextSplitter

r_splitter = RecursiveCharacterTextSplitter(separators=["\n\n", "\n"], chunk_size=180, chunk_overlap=0)

chunks = r_splitter.split_text(text)
len(chunks)

for chunk in chunks:
  print(len(chunk))

#How langchain did!

chunks = text.split("\n\n")
for chunk in chunks:
  print(len(chunk))

first_split = chunks[0]
first_split

second_split = first_split.split("\n")

for chunk in second_split:
  print(len(chunk))

!pip install faiss-chunk_overlap
!pip install sentence-transformers

import pandas as pd

pd.set_option('display.max_colwidth', 100)

df = pd.read_csv("/content/sample_text.csv")
df.head()

from sentence_transformers import SentenceTransformer

encoder = SentenceTransformer("all-mpnet-base-v2")
vectors = encoder.encode(df.text)
vectors.shape

vectors

dim = vectors.shape[1]
dim

!pip install faiss-cpu

import faiss

index = faiss.IndexFlatL2(dim)
index

index.add(vectors)
index

search_query="I want to buy a polo t-shirt"
vec = encoder.encode(search_query)
vec.shape

import numpy as np
svec = np.array(vec).reshape(1,-1)
svec.shape

distances, I = index.search(svec, k=2)

I

df.loc[I[0]]

search_query

df

!pip install streamlit

import os
import streamlit as st
import pickle
import time
import langchain
from langchain import OpenAI

from langchain.chains import RetrievalQAWithSourcesChain

from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain

from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain.document_loaders import UnstructuredURLLoader

from langchain.embeddings import OpenAIEmbeddings

from langchain.vectorstores import FAISS

os.environ['OPENAI_API_KEY'] = "add your key here"

!pip install openai

llm = OpenAI(temperature=0.9, max_tokens=500)

loaders = UnstructuredURLLoader(urls=["https://www.msn.com/en-in/news/India/the-real-reason-why-hardik-pandya-natasa-stankovic-separated-exclusive/ar-AA1pj7Jh?ocid=msedgntp&pc=ASTS&cvid=4777eab7e12441b4b98e6e6c901aabe7&ei=36", "https://www.msn.com/en-in/entertainment/other/katrina-kaif-reveals-what-hubby-vicky-kaushal-asks-her-to-do-at-dinner-table-tells-me-to-put-down/ar-AA1pfHrZ?ocid=msedgntp&pc=ASTS&cvid=4777eab7e12441b4b98e6e6c901aabe7&ei=45"])
data = loaders.load()
len(data)

text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=100)
docs = text_splitter.split_documents(data)
len(docs)

!pip install tiktoken

print(docs)
print(len(docs))

print(embeddings)

from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores.faiss import FAISS

# Sample documents
docs = ["Document 1 text", "Document 2 text", "Document 3 text"]

# Initialize the OpenAIEmbeddings
embeddings = OpenAIEmbeddings()

# Generate embeddings for the documents
doc_embeddings = embeddings.embed_documents(docs)

# Create the FAISS index using the documents and their corresponding embeddings
vectorindex_openai = FAISS.from_documents(docs, doc_embeddings)

embeddings = OpenAIEmbeddings()
vectorindex_openai = FAISS.from_documents(docs, embeddings)

if os.path.exists(file_path):
  with open(file_path, "rb") as f:
    vectorIndex = pickle.load(f)

chain = RetrievalQAWithSourcesChain.from_chain_type(llm=llm, retriever=vectorIndex.as_retriever())
chain

